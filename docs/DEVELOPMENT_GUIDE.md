# Development Guide

## üèóÔ∏è Architecture Overview

### System Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend      ‚îÇ    ‚îÇ   FastAPI       ‚îÇ    ‚îÇ   AI Services   ‚îÇ
‚îÇ   (HTML/CSS/JS) ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   Backend       ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   (Embeddings)  ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ Upload UI     ‚îÇ    ‚îÇ ‚Ä¢ PDF Upload    ‚îÇ    ‚îÇ ‚Ä¢ Sentence      ‚îÇ
‚îÇ ‚Ä¢ Chat Interface‚îÇ    ‚îÇ ‚Ä¢ Text Extract  ‚îÇ    ‚îÇ   Transformers  ‚îÇ
‚îÇ ‚Ä¢ Doc Management‚îÇ    ‚îÇ ‚Ä¢ API Endpoints ‚îÇ    ‚îÇ ‚Ä¢ Vector Search ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇ   Data Layer    ‚îÇ
                       ‚îÇ                 ‚îÇ
                       ‚îÇ ‚Ä¢ ChromaDB      ‚îÇ
                       ‚îÇ ‚Ä¢ File Storage  ‚îÇ
                       ‚îÇ ‚Ä¢ JSON Metadata ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow
1. **Upload**: PDF ‚Üí FastAPI ‚Üí Text Extraction ‚Üí Chunking ‚Üí Embeddings ‚Üí ChromaDB
2. **Query**: Question ‚Üí Embedding ‚Üí Vector Search ‚Üí Context Retrieval ‚Üí Answer Generation
3. **Management**: CRUD operations on documents and metadata

## üìÅ Project Structure

```
pdfmodel/
‚îú‚îÄ‚îÄ main.py                 # FastAPI application entry point
‚îú‚îÄ‚îÄ config.py              # Configuration management
‚îú‚îÄ‚îÄ logger_config.py       # Logging setup
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ .env.example          # Environment variables template
‚îú‚îÄ‚îÄ documents_metadata.json # Persistent document metadata
‚îÇ
‚îú‚îÄ‚îÄ models/               # Data models
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ schemas.py       # Pydantic models
‚îÇ
‚îú‚îÄ‚îÄ services/            # Business logic
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ pdf_processor.py  # PDF text extraction
‚îÇ   ‚îú‚îÄ‚îÄ embedding_service.py # Text embeddings
‚îÇ   ‚îî‚îÄ‚îÄ rag_service.py   # RAG pipeline
‚îÇ
‚îú‚îÄ‚îÄ database/            # Data access layer
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ vector_store.py  # ChromaDB operations
‚îÇ
‚îú‚îÄ‚îÄ static/              # Frontend assets
‚îÇ   ‚îú‚îÄ‚îÄ index.html       # Main UI
‚îÇ   ‚îú‚îÄ‚îÄ css/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ style.css    # Styling
‚îÇ   ‚îî‚îÄ‚îÄ js/
‚îÇ       ‚îî‚îÄ‚îÄ app.js       # Frontend logic
‚îÇ
‚îú‚îÄ‚îÄ uploads/             # PDF file storage
‚îú‚îÄ‚îÄ chroma_db/           # Vector database
‚îú‚îÄ‚îÄ logs/                # Application logs
‚îî‚îÄ‚îÄ docs/                # Documentation
    ‚îú‚îÄ‚îÄ API_DOCUMENTATION.md
    ‚îú‚îÄ‚îÄ DEVELOPMENT_GUIDE.md
    ‚îî‚îÄ‚îÄ DEPLOYMENT_GUIDE.md
```

## üîß Core Components

### 1. PDF Processing (`services/pdf_processor.py`)
- **Text Extraction**: Dual engine support (PyPDF2 + pdfplumber)
- **Chunking**: LangChain RecursiveCharacterTextSplitter
- **Validation**: File type and size checks
- **Metadata**: File info extraction

**Key Methods:**
```python
async def extract_text(file_path: str) -> str
def create_chunks(text: str) -> List[Document]
async def process_pdf(file_path: str, doc_id: str, filename: str) -> List[Document]
```

### 2. Embedding Service (`services/embedding_service.py`)
- **Model**: Sentence-transformers (all-MiniLM-L6-v2)
- **Batch Processing**: Memory-efficient embedding generation
- **Caching**: Optional embedding cache
- **Similarity**: Cosine similarity calculations

**Key Methods:**
```python
def encode_texts(texts: List[str]) -> List[List[float]]
def encode_query(query: str) -> List[float]
def encode_documents(documents: List[Document]) -> List[List[float]]
```

### 3. Vector Store (`database/vector_store.py`)
- **ChromaDB**: Persistent vector database
- **Operations**: Add, search, delete, list
- **Metadata**: Document and chunk metadata
- **Filtering**: Document-specific searches

**Key Methods:**
```python
def add_documents(documents: List[Document], embeddings: List[List[float]]) -> bool
def similarity_search(query_embedding: List[float], k: int) -> List[Dict]
def delete_document(document_id: str) -> bool
```

### 4. RAG Service (`services/rag_service.py`)
- **Pipeline**: End-to-end RAG implementation
- **Answer Generation**: Specialized extraction logic
- **Metadata Management**: Persistent document metadata
- **Question Types**: Names, skills, experience, contact, education

**Key Methods:**
```python
async def process_and_store_pdf(file_path: str, filename: str, doc_id: str) -> Dict
async def answer_question(question: str, doc_id: str, max_chunks: int) -> Dict
def list_documents() -> List[Dict]
```

### 5. Frontend (`static/`)
- **Upload Interface**: Drag & drop with progress tracking
- **Chat UI**: Real-time conversation interface
- **Document Management**: Visual cards with actions
- **API Integration**: RESTful communication with backend

**Key Classes:**
```javascript
class PDFQAApp {
    async uploadFile(file)
    async sendQuestion()
    async loadDocuments()
    addChatMessage(sender, content, metadata)
}
```

## üß™ Testing

### Manual Testing
```bash
# Start the server
python main.py

# Test endpoints
curl http://localhost:8000/health
curl -X POST -F "file=@test.pdf" http://localhost:8000/upload-pdf
curl -X POST -H "Content-Type: application/json" \
     -d '{"question": "test"}' http://localhost:8000/ask-question
```

### Unit Testing (Future Enhancement)
```python
# Example test structure
def test_pdf_extraction():
    processor = PDFProcessor()
    text = await processor.extract_text("test.pdf")
    assert len(text) > 0

def test_embedding_generation():
    service = EmbeddingService()
    embeddings = service.encode_texts(["test text"])
    assert len(embeddings[0]) == 384  # Model dimension
```

## üîß Configuration

### Environment Variables
```bash
# Application
APP_NAME="PDF Question-Answering API"
DEBUG=false
HOST="0.0.0.0"
PORT=8000

# File Processing
MAX_FILE_SIZE=52428800  # 50MB
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# AI/ML
EMBEDDING_MODEL="all-MiniLM-L6-v2"
SIMILARITY_THRESHOLD=0.1

# Paths
UPLOAD_DIR="./uploads"
VECTOR_STORE_PATH="./chroma_db"
LOG_FILE="./logs/app.log"
```

### Configuration Class (`config.py`)
```python
class Settings(BaseSettings):
    app_name: str = "PDF Question-Answering API"
    debug: bool = False
    embedding_model: str = "all-MiniLM-L6-v2"
    # ... other settings
```

## üìä Performance Considerations

### Memory Usage
- **Embeddings**: ~1.5MB model in memory
- **PDFs**: Processed in chunks, not loaded entirely
- **Vector Store**: ChromaDB handles memory management
- **Frontend**: Minimal JavaScript, no heavy frameworks

### Processing Speed
- **PDF Upload**: ~1-5 seconds depending on size
- **Embedding Generation**: ~100ms per chunk
- **Vector Search**: ~10-50ms
- **Answer Generation**: ~10-100ms

### Scaling
- **Horizontal**: Multiple FastAPI instances behind load balancer
- **Vertical**: Increase CPU/RAM for faster processing
- **Database**: ChromaDB can handle millions of vectors
- **Storage**: File system for PDFs, consider cloud storage for production

## üêõ Debugging

### Logging
```python
# Enable debug logging
LOG_LEVEL="DEBUG"

# Check logs
tail -f logs/app.log
```

### Debug Endpoints
```bash
# Check document chunks
curl http://localhost:8000/debug/chunks/{document_id}

# Test similarity search
curl -X POST -H "Content-Type: application/json" \
     -d '{"question": "test"}' http://localhost:8000/debug/search
```

### Common Issues
1. **"No documents found"**: Check if upload was successful
2. **"Not relevant enough"**: Lower similarity threshold
3. **Empty responses**: Verify text extraction worked
4. **Memory errors**: Reduce chunk size or batch size

## üîÑ Development Workflow

### 1. Setup
```bash
# Clone/create project
git clone <repo> && cd pdfmodel

# Create virtual environment
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt

# Setup environment
cp .env.example .env
```

### 2. Development
```bash
# Run in development mode
python main.py

# Frontend development
# Edit files in static/ directory
# No build process needed
```

### 3. Code Style
- **Python**: Follow PEP 8
- **JavaScript**: Use modern ES6+ features
- **CSS**: BEM methodology preferred
- **Comments**: Docstrings for all public methods

### 4. Version Control
```bash
# Ignore these files
echo "chroma_db/" >> .gitignore
echo "uploads/" >> .gitignore
echo "logs/" >> .gitignore
echo "documents_metadata.json" >> .gitignore
echo ".env" >> .gitignore
```

## üöÄ Future Enhancements

### Short Term
- [ ] Add user authentication
- [ ] Implement rate limiting
- [ ] Add more file formats (Word, TXT)
- [ ] Improve error handling

### Medium Term
- [ ] Multi-language support
- [ ] Advanced question types
- [ ] Document comparison
- [ ] Export functionality

### Long Term
- [ ] Real LLM integration (GPT, Claude)
- [ ] Advanced analytics
- [ ] Multi-user support
- [ ] Cloud deployment templates

## üß© Extension Points

### Adding New Question Types
```python
# In rag_service.py
def _extract_custom_info(self, content: str, question: str) -> str:
    # Custom extraction logic
    return "Custom answer"

# In _simple_answer_generation method
elif "custom_keyword" in question_lower:
    answer = self._extract_custom_info(combined_content, question)
```

### Custom Embedding Models
```python
# In embedding_service.py
def __init__(self, model_name: str = "custom-model"):
    self.model = SentenceTransformer(model_name)
```

### Additional File Types
```python
# In pdf_processor.py
class DocumentProcessor:  # Rename from PDFProcessor
    def extract_text_from_docx(self, file_path: str) -> str:
        # Word document processing
        pass
```